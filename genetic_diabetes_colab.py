# -*- coding: utf-8 -*-
"""Genetic_Diabetes_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YrBpDy1A8vEMnEVw3SjIZ3KctNdgJ2TZ
"""

!pip install deap

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

from deap import base, creator, tools, algorithms
import random
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/diabetes_ga_svm_project/data/diabetes.csv")

# Features & labels
X = data.drop("Outcome", axis=1)
y = data["Outcome"]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

default_model = SVC()
default_model.fit(X_train, y_train)
default_preds = default_model.predict(X_test)
default_acc = accuracy_score(y_test, default_preds)
print(f"Accuracy without GA: {default_acc:.4f}")

# # Genetic Algorithm for SVM hyperparameter tuning
# C_range = [0.1, 100]
# kernel_map = {0: 'linear', 1: 'poly', 2: 'rbf', 3: 'sigmoid'}

# creator.create("FitnessMax", base.Fitness, weights=(1.0,))
# creator.create("Individual", list, fitness=creator.FitnessMax)

# toolbox = base.Toolbox()
# toolbox.register("C", random.uniform, C_range[0], C_range[1])
# toolbox.register("kernel_idx", random.randint, 0, len(kernel_map) - 1)
# toolbox.register("individual", tools.initCycle, creator.Individual, (toolbox.C, toolbox.kernel_idx), n=1)
# toolbox.register("population", tools.initRepeat, list, toolbox.individual)
kernel_map = {0: 'linear', 1: 'poly', 2: 'rbf', 3: 'sigmoid'}

# Define fitness function
def eval_svm(individual):
    C = max(0.01, float(individual[0]))  # Ensure C is valid
    kernel_idx = int(round(individual[1]))
    kernel = kernel_map.get(kernel_idx, 'rbf')

    model = SVC(C=C, kernel=kernel)
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    return (acc,)

# Mutation function that keeps values in range
def mutate_and_repair(individual):
    tools.mutGaussian(individual, mu=0, sigma=1, indpb=0.2)
    individual[0] = min(max(individual[0], 0.01), 10.0)  # Clamp C
    individual[1] = int(round(min(max(individual[1], 0), 3)))  # Clamp kernel idx
    return individual,

# Create DEAP types (safe to call only once)
if not hasattr(creator, "FitnessMax"):
    creator.create("FitnessMax", base.Fitness, weights=(1.0,))
if not hasattr(creator, "Individual"):
    creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("C", random.uniform, 0.01, 10.0)
toolbox.register("kernel_idx", random.randint, 0, 3)
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.C, toolbox.kernel_idx), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

toolbox.register("evaluate", eval_svm)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", mutate_and_repair)
toolbox.register("select", tools.selTournament, tournsize=3)

# def eval_svm(individual):
#     C = individual[0]
#     kernel_idx = int(round(individual[1]))  # Convert to int
#     kernel_idx = max(0, min(kernel_idx, len(kernel_map) - 1))  # Keep in range
#     kernel = kernel_map[kernel_idx]

#     model = SVC(C=C, kernel=kernel)
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     accuracy = accuracy_score(y_test, predictions)
#     return (accuracy,)

# Fix mutation issues: keep values in valid range
# def check_bounds(individual):
#     individual[0] = min(max(individual[0], 0.01), 10.0)          # Clamp C between 0.01 and 10.0
#     individual[1] = int(round(min(max(individual[1], 0), 3)))    # Ensure kernel index is 0,1,2,3
#     return individual

# def mutate_and_repair(individual):
#     tools.mutGaussian(individual, mu=0, sigma=1, indpb=0.2)
#     return check_bounds(individual),

# toolbox.register("evaluate", eval_svm)
# toolbox.register("mate", tools.cxBlend, alpha=0.5)
# toolbox.register("mutate", mutate_and_repair)
# toolbox.register("select", tools.selTournament, tournsize=3)

# from deap import base, creator, tools, algorithms
# import random

# def run_ga():
#     # Create types only once
#     if not hasattr(creator, "FitnessMax"):
#         creator.create("FitnessMax", base.Fitness, weights=(1.0,))
#     if not hasattr(creator, "Individual"):
#         creator.create("Individual", list, fitness=creator.FitnessMax)

#     toolbox = base.Toolbox()

#     # Search space: C and kernel index
#     toolbox.register("C", random.uniform, 0.01, 10.0)
#     toolbox.register("kernel_idx", random.randint, 0, 3)  # 0-linear, 1-poly, 2-rbf, 3-sigmoid

#     toolbox.register("individual", tools.initCycle, creator.Individual,
#                      (toolbox.C, toolbox.kernel_idx), n=1)
#     toolbox.register("population", tools.initRepeat, list, toolbox.individual)

#     toolbox.register("evaluate", eval_svm)
#     toolbox.register("mate", tools.cxBlend, alpha=0.5)
#     toolbox.register("mutate", mutate_and_repair)
#     toolbox.register("select", tools.selTournament, tournsize=3)

#     # GA parameters
#     population = toolbox.population(n=10)
#     hof = tools.HallOfFame(1)

#     stats = tools.Statistics(lambda ind: ind.fitness.values[0])
#     stats.register("avg", lambda x: sum(x) / len(x))
#     stats.register("max", max)
#     stats.register("min", min)

#     # Run genetic algorithm
#     population, logbook = algorithms.eaSimple(population, toolbox,
#                                               cxpb=0.6, mutpb=0.3,
#                                               ngen=10, stats=stats,
#                                               halloffame=hof,
#                                               verbose=True)

#     # Save best result
#     best_ind = hof[0]
#     best_kernel = kernel_map[int(round(best_ind[1]))]

#     import os
#     os.makedirs("results", exist_ok=True)
#     with open("results/best_params.txt", "w") as f:
#         f.write(f"Best Accuracy: {best_ind.fitness.values[0]:.4f}\n")
#         f.write(f"Best C: {best_ind[0]:.4f}\n")
#         f.write(f"Best Kernel: {best_kernel}\n")

#     return logbook

# Run GA
population = toolbox.population(n=10)
hof = tools.HallOfFame(1)

stats = tools.Statistics(lambda ind: ind.fitness.values[0])
stats.register("avg", np.mean)
stats.register("max", np.max)
stats.register("min", np.min)

population, logbook = algorithms.eaSimple(population, toolbox,
                                          cxpb=0.6, mutpb=0.3,
                                          ngen=10, stats=stats,
                                          halloffame=hof,
                                          verbose=True)

# Extract best result
best_ind = hof[0]
best_kernel = kernel_map[int(round(best_ind[1]))]
best_acc = best_ind.fitness.values[0]

print(f"\nAccuracy WITH GA: {best_acc:.4f}")
print(f"Best C: {best_ind[0]:.4f}")
print(f"Best Kernel: {best_kernel}")

# Save results
os.makedirs("results", exist_ok=True)
with open("results/best_params.txt", "w") as f:
    f.write(f"Accuracy without GA: {default_acc:.4f}\n")
    f.write(f"Accuracy with GA: {best_acc:.4f}\n")
    f.write(f"Best C: {best_ind[0]:.4f}\n")
    f.write(f"Best Kernel: {best_kernel}\n")

# Plot GA performance
gen = logbook.select("gen")
acc = logbook.select("max")
plt.plot(gen, acc, marker='o')
plt.title("GA Performance Over Generations")
plt.xlabel("Generation")
plt.ylabel("Best Accuracy")
plt.grid(True)
plt.savefig("results/accuracy_plot.png")
plt.show()

